use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::sync::atomic::{AtomicU64, Ordering};
use std::time::Duration;

use crate::llm::client::types::TokenUsage;

/// ç¼“å­˜æ€§èƒ½ç›‘æ§å™¨
#[derive(Clone)]
pub struct CachePerformanceMonitor {
    metrics: Arc<CacheMetrics>,
}

/// ç¼“å­˜æŒ‡æ ‡
#[derive(Default)]
pub struct CacheMetrics {
    /// ç¼“å­˜å‘½ä¸­æ¬¡æ•°
    pub cache_hits: AtomicU64,
    /// ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°
    pub cache_misses: AtomicU64,
    /// ç¼“å­˜å†™å…¥æ¬¡æ•°
    pub cache_writes: AtomicU64,
    /// ç¼“å­˜é”™è¯¯æ¬¡æ•°
    pub cache_errors: AtomicU64,
    /// æ€»èŠ‚çœçš„æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
    pub total_inference_time_saved: AtomicU64,
    /// æ€»èŠ‚çœçš„æ¨ç†æˆæœ¬ï¼ˆä¼°ç®—ï¼‰
    pub total_cost_saved: AtomicU64,
    /// æ€»èŠ‚çœçš„è¾“å…¥tokenæ•°é‡
    pub total_input_tokens_saved: AtomicU64,
    /// æ€»èŠ‚çœçš„è¾“å‡ºtokenæ•°é‡
    pub total_output_tokens_saved: AtomicU64,
}

/// ç¼“å­˜æ€§èƒ½æŠ¥å‘Š
#[derive(Debug, Serialize, Deserialize)]
pub struct CachePerformanceReport {
    /// ç¼“å­˜å‘½ä¸­ç‡
    pub hit_rate: f64,
    /// æ€»ç¼“å­˜æ“ä½œæ¬¡æ•°
    pub total_operations: u64,
    /// ç¼“å­˜å‘½ä¸­æ¬¡æ•°
    pub cache_hits: u64,
    /// ç¼“å­˜æœªå‘½ä¸­æ¬¡æ•°
    pub cache_misses: u64,
    /// ç¼“å­˜å†™å…¥æ¬¡æ•°
    pub cache_writes: u64,
    /// ç¼“å­˜é”™è¯¯æ¬¡æ•°
    pub cache_errors: u64,
    /// èŠ‚çœçš„æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
    pub inference_time_saved: f64,
    /// èŠ‚çœçš„æ¨ç†æˆæœ¬ï¼ˆç¾å…ƒï¼Œä¼°ç®—ï¼‰
    pub cost_saved: f64,
    /// æ€§èƒ½æå‡ç™¾åˆ†æ¯”
    pub performance_improvement: f64,
    /// èŠ‚çœçš„è¾“å…¥tokenæ•°é‡
    pub input_tokens_saved: u64,
    /// èŠ‚çœçš„è¾“å‡ºtokenæ•°é‡
    pub output_tokens_saved: u64,
    /// åˆ†ç±»ç»Ÿè®¡
    pub category_stats: HashMap<String, CategoryPerformanceStats>,
}

/// åˆ†ç±»æ€§èƒ½ç»Ÿè®¡
#[derive(Debug, Serialize, Deserialize)]
pub struct CategoryPerformanceStats {
    pub hits: u64,
    pub misses: u64,
    pub hit_rate: f64,
    pub time_saved: f64,
    pub cost_saved: f64,
}

impl CachePerformanceMonitor {
    pub fn new() -> Self {
        Self {
            metrics: Arc::new(CacheMetrics::default()),
        }
    }

    /// è®°å½•ç¼“å­˜å‘½ä¸­
    pub fn record_cache_hit(
        &self,
        category: &str,
        inference_time_saved: Duration,
        token_usage: TokenUsage,
        model_name: &str,
    ) {
        self.metrics.cache_hits.fetch_add(1, Ordering::Relaxed);
        self.metrics
            .total_inference_time_saved
            .fetch_add(inference_time_saved.as_millis() as u64, Ordering::Relaxed);

        // è®°å½•èŠ‚çœçš„tokenæ•°é‡
        self.metrics
            .total_input_tokens_saved
            .fetch_add(token_usage.input_tokens, Ordering::Relaxed);
        self.metrics
            .total_output_tokens_saved
            .fetch_add(token_usage.output_tokens, Ordering::Relaxed);

        // åŸºäºå®é™…tokenä½¿ç”¨æƒ…å†µè®¡ç®—èŠ‚çœçš„æˆæœ¬
        let estimated_cost_saved = token_usage.estimate_cost(model_name);
        self.metrics.total_cost_saved.fetch_add(
            (estimated_cost_saved * 1000.0) as u64, // å­˜å‚¨ä¸ºæ¯«ç¾å…ƒ
            Ordering::Relaxed,
        );

        println!(
            "   ğŸ’° ç¼“å­˜å‘½ä¸­ [{}] - èŠ‚çœæ¨ç†æ—¶é—´: {:.2}ç§’, èŠ‚çœtokens: {}è¾“å…¥+{}è¾“å‡º, ä¼°ç®—èŠ‚çœæˆæœ¬: ${:.4}",
            category,
            inference_time_saved.as_secs_f64(),
            token_usage.input_tokens,
            token_usage.output_tokens,
            estimated_cost_saved
        );
    }

    /// è®°å½•ç¼“å­˜æœªå‘½ä¸­
    pub fn record_cache_miss(&self, category: &str) {
        self.metrics.cache_misses.fetch_add(1, Ordering::Relaxed);
        println!("   ğŸ” ç¼“å­˜æœªå‘½ä¸­ [{}] - éœ€è¦è¿›è¡ŒAIæ¨ç†", category);
    }

    /// è®°å½•ç¼“å­˜å†™å…¥
    pub fn record_cache_write(&self, category: &str) {
        self.metrics.cache_writes.fetch_add(1, Ordering::Relaxed);
        println!("   ğŸ’¾ ç¼“å­˜å†™å…¥ [{}] - ç»“æœå·²ç¼“å­˜", category);
    }

    /// è®°å½•ç¼“å­˜é”™è¯¯
    pub fn record_cache_error(&self, category: &str, error: &str) {
        self.metrics.cache_errors.fetch_add(1, Ordering::Relaxed);
        eprintln!("   âŒ ç¼“å­˜é”™è¯¯ [{}]: {}", category, error);
    }

    /// ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    pub fn generate_report(&self) -> CachePerformanceReport {
        let hits = self.metrics.cache_hits.load(Ordering::Relaxed);
        let misses = self.metrics.cache_misses.load(Ordering::Relaxed);
        let writes = self.metrics.cache_writes.load(Ordering::Relaxed);
        let errors = self.metrics.cache_errors.load(Ordering::Relaxed);
        let total_operations = hits + misses;

        let hit_rate = if total_operations > 0 {
            hits as f64 / total_operations as f64
        } else {
            0.0
        };

        let inference_time_saved = self
            .metrics
            .total_inference_time_saved
            .load(Ordering::Relaxed) as f64
            / 1000.0; // è½¬æ¢ä¸ºç§’
        let cost_saved = self.metrics.total_cost_saved.load(Ordering::Relaxed) as f64 / 1000.0; // è½¬æ¢ä¸ºç¾å…ƒ

        let input_tokens_saved = self
            .metrics
            .total_input_tokens_saved
            .load(Ordering::Relaxed);
        let output_tokens_saved = self
            .metrics
            .total_output_tokens_saved
            .load(Ordering::Relaxed);

        let performance_improvement = if misses > 0 {
            (hits as f64 / (hits + misses) as f64) * 100.0
        } else {
            0.0
        };

        CachePerformanceReport {
            hit_rate,
            total_operations,
            cache_hits: hits,
            cache_misses: misses,
            cache_writes: writes,
            cache_errors: errors,
            inference_time_saved,
            cost_saved,
            performance_improvement,
            input_tokens_saved,
            output_tokens_saved,
            category_stats: HashMap::new(), // TODO: å®ç°åˆ†ç±»ç»Ÿè®¡
        }
    }
}

impl Default for CachePerformanceMonitor {
    fn default() -> Self {
        Self::new()
    }
}
