use crate::llm::LLMClient;
use anyhow::Result;
use serde::{Deserialize, Serialize};
use std::time::Instant;

use crate::agents::preprocessing_agent::PreprocessingResult;
use crate::cache::CacheManager;
use crate::config::Config;
use crate::extractors::{ResearchExtractor, ResearchReport};

/// è°ƒç ”Agent
pub struct ResearchAgent {
    llm_client: LLMClient,
    config: Config,
    cache_manager: CacheManager,
    research_extractor: ResearchExtractor,
}

/// è°ƒç ”ç»“æœ
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct ResearchResult {
    pub reports: Vec<ResearchReport>,
    pub insights: Vec<String>,
    pub recommendations: Vec<String>,
    pub processing_time: f64,
    pub summary: String,
}

impl ResearchAgent {
    pub async fn new(config: Config) -> Result<Self> {
        let llm_client = LLMClient::new(config.llm.clone())?;
        let cache_manager = CacheManager::new(config.cache.clone());
        let research_extractor = ResearchExtractor::new(cache_manager.clone());

        Ok(Self {
            llm_client,
            config,
            cache_manager,
            research_extractor,
        })
    }

    /// ç”Ÿæˆè°ƒç ”æ–‡æ¡£
    pub async fn generate_research(
        &self,
        preprocessing_result: &PreprocessingResult,
    ) -> Result<ResearchResult> {
        let start_time = Instant::now();

        println!("ğŸ” å¼€å§‹è°ƒç ”æ–‡æ¡£ç”Ÿæˆ...");

        // 1. ç”ŸæˆåŸºç¡€è°ƒç ”æŠ¥å‘Š
        println!("ğŸ“Š ç”ŸæˆåŸºç¡€è°ƒç ”æŠ¥å‘Š...");
        let mut reports = self
            .research_extractor
            .generate_reports(preprocessing_result)
            .await?;

        // 2. ä½¿ç”¨AIå¢å¼ºè°ƒç ”æŠ¥å‘Š
        println!("ğŸ¤– ä½¿ç”¨AIå¢å¼ºè°ƒç ”æŠ¥å‘Š...");
        for report in &mut reports {
            if let Ok(enhanced_report) = self
                .enhance_report_with_ai(report, preprocessing_result)
                .await
            {
                *report = enhanced_report;
            }
        }

        // 3. ç”Ÿæˆç»¼åˆæ´å¯Ÿ
        println!("ğŸ’¡ ç”Ÿæˆç»¼åˆæ´å¯Ÿ...");
        let insights = self
            .generate_comprehensive_insights(&reports, preprocessing_result)
            .await?;

        // 4. ç”Ÿæˆæ”¹è¿›å»ºè®®
        println!("ğŸ“ ç”Ÿæˆæ”¹è¿›å»ºè®®...");
        let recommendations = self
            .generate_recommendations(&reports, preprocessing_result)
            .await?;

        // 5. ç”Ÿæˆæ‘˜è¦
        let summary = self.generate_research_summary(&reports, &insights, &recommendations);

        let processing_time = start_time.elapsed().as_secs_f64();

        println!("âœ… è°ƒç ”æ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {:.2}ç§’", processing_time);

        Ok(ResearchResult {
            reports,
            insights,
            recommendations,
            processing_time,
            summary,
        })
    }

    async fn enhance_report_with_ai(
        &self,
        report: &ResearchReport,
        preprocessing_result: &PreprocessingResult,
    ) -> Result<ResearchReport> {
        // æ„å»ºAIå¢å¼ºæç¤º
        let prompt = self.build_research_enhancement_prompt(report, preprocessing_result);

        // å°è¯•ä»ç¼“å­˜è·å– - ç›´æ¥ä½¿ç”¨promptä½œä¸ºkeyï¼ŒCacheManagerä¼šè‡ªåŠ¨è®¡ç®—hash
        if let Some(cached_report) = self
            .cache_manager
            . get::<ResearchReport>("ai_research", &prompt)
            .await?
        {
            println!("   âœ… ä½¿ç”¨ç¼“å­˜çš„AIè°ƒç ”ç»“æœ: {}", report.title);
            return Ok(cached_report);
        }

        println!("   ğŸ¤– æ­£åœ¨è¿›è¡ŒAIè°ƒç ”åˆ†æ: {}", report.title);

        // æ‰§è¡ŒAIåˆ†æ
        let system_msg =
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è½¯ä»¶æ¶æ„ç ”ç©¶å‘˜ï¼Œä¸“é—¨æ·±å…¥åˆ†æè½¯ä»¶é¡¹ç›®çš„æ¶æ„ã€è®¾è®¡å’Œè´¨é‡ã€‚".to_string();
        let prompt_clone = prompt.clone();
        let ai_response = self
            .llm_client
            .prompt(&system_msg, &prompt_clone)
            .await
            .map_err(|e| anyhow::anyhow!("AIåˆ†æå¤±è´¥: {}", e))?;

        // è§£æAIå“åº”å¹¶å¢å¼ºæŠ¥å‘Š
        let mut enhanced_report = report.clone();
        self.parse_ai_research_response(&ai_response, &mut enhanced_report);

        // ç¼“å­˜ç»“æœ - ç›´æ¥ä½¿ç”¨promptä½œä¸ºkey
        self.cache_manager
            .set("ai_research", &prompt, &enhanced_report)
            .await?;

        Ok(enhanced_report)
    }

    fn build_research_enhancement_prompt(
        &self,
        report: &ResearchReport,
        preprocessing_result: &PreprocessingResult,
    ) -> String {
        format!(
            r#"
è¯·æ·±å…¥åˆ†æä»¥ä¸‹è½¯ä»¶é¡¹ç›®çš„è°ƒç ”æŠ¥å‘Šï¼Œå¹¶æä¾›ä¸“ä¸šçš„å¢å¼ºåˆ†æï¼š

## é¡¹ç›®åŸºæœ¬ä¿¡æ¯
- æ–‡ä»¶æ€»æ•°: {}
- æ ¸å¿ƒç»„ä»¶æ•°: {}
- ä¸»è¦æŠ€æœ¯æ ˆ: {}

## å½“å‰è°ƒç ”æŠ¥å‘Š
**æ ‡é¢˜**: {}
**ç±»å‹**: {}
**æ‘˜è¦**: {}

**ç°æœ‰æ´å¯Ÿ**:
{}

**ç°æœ‰å»ºè®®**:
{}

## è¯·æä¾›ä»¥ä¸‹æ·±åº¦åˆ†æï¼š

1. **æ·±åº¦æ´å¯Ÿ**: åŸºäºé¡¹ç›®ç‰¹å¾ï¼Œæä¾›3-5ä¸ªæ·±å±‚æ¬¡çš„æŠ€æœ¯æ´å¯Ÿ
2. **æ¶æ„è¯„ä¼°**: è¯„ä¼°å½“å‰æ¶æ„çš„ä¼˜åŠ¿å’Œæ½œåœ¨é—®é¢˜
3. **æŠ€æœ¯å€ºåŠ¡**: è¯†åˆ«å¯èƒ½å­˜åœ¨çš„æŠ€æœ¯å€ºåŠ¡å’Œé£é™©ç‚¹
4. **æ”¹è¿›è·¯å¾„**: æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®å’Œå®æ–½è·¯å¾„
5. **æœ€ä½³å®è·µ**: æ¨èç›¸å…³çš„æœ€ä½³å®è·µå’Œè®¾è®¡æ¨¡å¼

è¯·ç”¨ç»“æ„åŒ–çš„æ ¼å¼å›ç­”ï¼Œæ¯ä¸ªéƒ¨åˆ†ç”¨æ˜ç¡®çš„æ ‡é¢˜åˆ†éš”ã€‚
"#,
            preprocessing_result.project_structure.total_files,
            preprocessing_result.core_components.len(),
            preprocessing_result
                .project_structure
                .file_types
                .keys()
                .take(3)
                .cloned()
                .collect::<Vec<_>>()
                .join(", "),
            report.title,
            report.report_type,
            report.summary,
            report.insights.join("\n- "),
            report.recommendations.join("\n- ")
        )
    }

    fn parse_ai_research_response(&self, response: &str, report: &mut ResearchReport) {
        // è§£ææ·±åº¦æ´å¯Ÿ
        if let Some(insights_start) = response.find("æ·±åº¦æ´å¯Ÿ") {
            if let Some(insights_end) = response[insights_start..].find("\n\n") {
                let insights_text = &response[insights_start..insights_start + insights_end];
                let new_insights: Vec<String> = insights_text
                    .lines()
                    .skip(1)
                    .filter_map(|line| {
                        let line = line.trim();
                        if line.starts_with('-')
                            || line.starts_with('â€¢')
                            || line.chars().next().map_or(false, |c| c.is_numeric())
                        {
                            Some(
                                line.trim_start_matches('-')
                                    .trim_start_matches('â€¢')
                                    .trim_start_matches(char::is_numeric)
                                    .trim_start_matches('.')
                                    .trim()
                                    .to_string(),
                            )
                        } else {
                            None
                        }
                    })
                    .collect();

                if !new_insights.is_empty() {
                    report.insights.extend(new_insights);
                }
            }
        }

        // è§£ææ”¹è¿›å»ºè®®
        if let Some(improvements_start) = response.find("æ”¹è¿›è·¯å¾„") {
            if let Some(improvements_end) = response[improvements_start..].find("\n\n") {
                let improvements_text =
                    &response[improvements_start..improvements_start + improvements_end];
                let new_recommendations: Vec<String> = improvements_text
                    .lines()
                    .skip(1)
                    .filter_map(|line| {
                        let line = line.trim();
                        if line.starts_with('-')
                            || line.starts_with('â€¢')
                            || line.chars().next().map_or(false, |c| c.is_numeric())
                        {
                            Some(
                                line.trim_start_matches('-')
                                    .trim_start_matches('â€¢')
                                    .trim_start_matches(char::is_numeric)
                                    .trim_start_matches('.')
                                    .trim()
                                    .to_string(),
                            )
                        } else {
                            None
                        }
                    })
                    .collect();

                if !new_recommendations.is_empty() {
                    report.recommendations.extend(new_recommendations);
                }
            }
        }

        // æ›´æ–°å†…å®¹
        if let Some(content_start) = response.find("æ¶æ„è¯„ä¼°") {
            if let Some(content_end) = response[content_start..].find("\n\n") {
                let content = response[content_start..content_start + content_end]
                    .lines()
                    .skip(1)
                    .collect::<Vec<_>>()
                    .join("\n")
                    .trim()
                    .to_string();
                if !content.is_empty() {
                    report.content = format!("{}\n\n## AIå¢å¼ºåˆ†æ\n{}", report.content, content);
                }
            }
        }
    }

    async fn generate_comprehensive_insights(
        &self,
        reports: &[ResearchReport],
        preprocessing_result: &PreprocessingResult,
    ) -> Result<Vec<String>> {
        let mut insights = Vec::new();

        // ç»¼åˆæ‰€æœ‰æŠ¥å‘Šçš„æ´å¯Ÿ
        for report in reports {
            insights.extend(report.insights.clone());
        }

        // æ·»åŠ è·¨æŠ¥å‘Šçš„ç»¼åˆæ´å¯Ÿ
        insights.push(format!("é¡¹ç›®åŒ…å« {} ä¸ªè°ƒç ”ç»´åº¦çš„æ·±åº¦åˆ†æ", reports.len()));

        // åŸºäºç»„ä»¶è´¨é‡çš„æ´å¯Ÿ
        let avg_quality = if !preprocessing_result.component_analyses.is_empty() {
            preprocessing_result
                .component_analyses
                .iter()
                .map(|a| a.quality_assessment.overall_score)
                .sum::<f64>()
                / preprocessing_result.component_analyses.len() as f64
        } else {
            0.0
        };

        insights.push(format!(
            "æ•´ä½“ä»£ç è´¨é‡{}ï¼Œå¹³å‡åˆ†æ•° {:.1}/10",
            if avg_quality > 0.7 {
                "ä¼˜ç§€"
            } else if avg_quality > 0.5 {
                "è‰¯å¥½"
            } else {
                "éœ€è¦æ”¹è¿›"
            },
            avg_quality * 10.0
        ));

        // æ¶æ„å¤æ‚åº¦æ´å¯Ÿ
        let total_complexity: f64 = preprocessing_result
            .component_analyses
            .iter()
            .map(|a| a.complexity_metrics.cyclomatic_complexity)
            .sum();
        let avg_complexity = if !preprocessing_result.component_analyses.is_empty() {
            total_complexity / preprocessing_result.component_analyses.len() as f64
        } else {
            0.0
        };

        insights.push(format!(
            "å¹³å‡åœˆå¤æ‚åº¦ä¸º {:.1}ï¼Œ{}",
            avg_complexity,
            if avg_complexity > 10.0 {
                "å»ºè®®é‡æ„å¤æ‚å‡½æ•°"
            } else if avg_complexity > 5.0 {
                "å¤æ‚åº¦é€‚ä¸­"
            } else {
                "ä»£ç ç»“æ„ç®€æ´"
            }
        ));

        Ok(insights)
    }

    async fn generate_recommendations(
        &self,
        reports: &[ResearchReport],
        preprocessing_result: &PreprocessingResult,
    ) -> Result<Vec<String>> {
        let mut recommendations = Vec::new();

        // ç»¼åˆæ‰€æœ‰æŠ¥å‘Šçš„å»ºè®®
        for report in reports {
            recommendations.extend(report.recommendations.clone());
        }

        // æ·»åŠ åŸºäºæ•´ä½“åˆ†æçš„å»ºè®®
        if preprocessing_result.core_components.len() > 20 {
            recommendations.push("è€ƒè™‘è¿›ä¸€æ­¥æ¨¡å—åŒ–ï¼Œå°†ç›¸å…³ç»„ä»¶ç»„ç»‡åˆ°å­æ¨¡å—ä¸­".to_string());
        }

        if preprocessing_result.project_structure.total_files > 100 {
            recommendations.push("å»ºè®®å»ºç«‹æ¸…æ™°çš„ä»£ç ç»„ç»‡è§„èŒƒå’Œæ–‡æ¡£".to_string());
        }

        // åŸºäºè´¨é‡åˆ†æçš„å»ºè®®
        let low_quality_components = preprocessing_result
            .component_analyses
            .iter()
            .filter(|a| a.quality_assessment.overall_score < 0.5)
            .count();

        if low_quality_components > 0 {
            recommendations.push(format!(
                "ä¼˜å…ˆé‡æ„ {} ä¸ªè´¨é‡è¾ƒä½çš„ç»„ä»¶",
                low_quality_components
            ));
        }

        // å»é‡
        recommendations.sort();
        recommendations.dedup();

        Ok(recommendations)
    }

    fn generate_research_summary(
        &self,
        reports: &[ResearchReport],
        insights: &[String],
        recommendations: &[String],
    ) -> String {
        format!(
            r#"è°ƒç ”æ–‡æ¡£ç”Ÿæˆæ‘˜è¦:

ğŸ“Š è°ƒç ”æŠ¥å‘Š:
- ç”ŸæˆæŠ¥å‘Šæ•°: {}
- ä¸»è¦æŠ¥å‘Šç±»å‹: {}

ğŸ’¡ å…³é”®æ´å¯Ÿ:
- æ€»æ´å¯Ÿæ•°: {}
- æ ¸å¿ƒå‘ç°: {}

ğŸ“ æ”¹è¿›å»ºè®®:
- æ€»å»ºè®®æ•°: {}
- ä¼˜å…ˆå»ºè®®: {}

ğŸ¯ è°ƒç ”ç»“è®º:
é¡¹ç›®æ•´ä½“{}ï¼Œå»ºè®®é‡ç‚¹å…³æ³¨{}ã€‚"#,
            reports.len(),
            reports
                .iter()
                .map(|r| r.report_type.as_str())
                .collect::<Vec<_>>()
                .join(", "),
            insights.len(),
            insights.first().unwrap_or(&"æ— ".to_string()),
            recommendations.len(),
            recommendations.first().unwrap_or(&"æ— ".to_string()),
            if reports.iter().any(|r| r.priority > 0.8) {
                "æ¶æ„è®¾è®¡è‰¯å¥½"
            } else {
                "æœ‰æ”¹è¿›ç©ºé—´"
            },
            if recommendations.len() > 3 {
                "ä»£ç è´¨é‡æå‡"
            } else {
                "æ¶æ„ä¼˜åŒ–"
            }
        )
    }
}