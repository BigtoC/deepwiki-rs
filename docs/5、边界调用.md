# 系统边界接口文档

本文档描述了系统的外部调用接口，包括CLI命令、API端点、配置参数等边界机制。

## 命令行接口 (CLI)

### litho (deepwiki-rs)

**描述**: AI-based high-performance generation engine for documentation, It can intelligently analyze project structures, identify core modules, and generate professional architecture documentation.

**源文件**: `src/cli.rs`

**参数**:

- `project_path` (PathBuf): 必需 - 项目路径
- `output_path` (PathBuf): 必需 - 输出路径

**选项**:

- `config, c`(Option<PathBuf>): 可选 - 配置文件路径
- `name, n`(Option<String>): 可选 - 项目名称
- `skip_preprocessing`(bool): 可选 - 是否跳过项目预处理 (默认: `false`)
- `skip_research`(bool): 可选 - 是否跳过调研文档生成 (默认: `false`)
- `skip_documentation`(bool): 可选 - 是否跳过最终文档生成 (默认: `false`)
- `verbose, v`(bool): 可选 - 是否启用详细日志 (默认: `false`)
- `model_efficient`(Option<String>): 可选 - 高能效模型，优先用于Litho引擎的常规推理任务
- `model_powerful`(Option<String>): 可选 - 高质量模型，优先用于Litho引擎的复杂推理任务，以及作为efficient失效情况下的兜底
- `llm_api_base_url`(Option<String>): 可选 - LLM API基地址
- `llm_api_key`(Option<String>): 可选 - LLM API KEY
- `max_tokens`(Option<u32>): 可选 - 最大tokens数
- `temperature`(Option<f64>): 可选 - 温度参数
- `max_parallels`(Option<usize>): 可选 - 最大并行请求数
- `llm_provider`(Option<String>): 可选 - LLM Provider (openai, mistral, openrouter, anthropic, deepseek)
- `target_language`(Option<String>): 可选 - 目标语言 (zh, en, ja, ko, de, fr, ru)
- `disable_preset_tools`(bool): 可选 - 生成报告后,自动使用报告助手查看报告 (默认: `false`)
- `no_cache`(bool): 可选 - 是否禁用缓存
- `force_regenerate`(bool): 可选 - 强制重新生成（清除缓存）

**使用示例**:

```bash
litho --project_path ./my-project --output_path ./docs --verbose
```

```bash
litho -p ./backend -o ./architecture-docs --llm_provider openai --model_efficient gpt-3.5-turbo
```

```bash
litho --config ./litho.toml --skip_preprocessing --name "My Awesome Project"
```

```bash
litho --project_path ./rust-app --target_language zh --no_cache
```

## API接口

### POST /extract

**描述**: 从文本中提取结构化数据，利用JSON Schema进行类型安全的数据解析

**源文件**: `src/llm/client/mod.rs`

**请求格式**: system_prompt: &str, user_prompt: &str

**响应格式**: Result<T> where T: JsonSchema + Deserialize + Serialize

### POST /prompt

**描述**: 执行智能对话（使用默认ReAct配置），支持多轮推理和工具调用

**源文件**: `src/llm/client/mod.rs`

**请求格式**: system_prompt: &str, user_prompt: &str

**响应格式**: Result<String>

### POST /prompt_with_react

**描述**: 使用ReAct模式进行多轮对话，支持工具调用与总结推理fallover机制

**源文件**: `src/llm/client/mod.rs`

**请求格式**: system_prompt: &str, user_prompt: &str, react_config: ReActConfig

**响应格式**: Result<ReActResponse>

### POST /prompt_without_react

**描述**: 执行不使用工具的单轮对话，适用于简单问答场景

**源文件**: `src/llm/client/mod.rs`

**请求格式**: system_prompt: &str, user_prompt: &str

**响应格式**: Result<String>

## 集成建议

### CLI集成

如何在自动化流程中集成DeepWiki-RS命令行工具

**示例代码**:

```
#!/bin/bash
# 自动化文档生成脚本
PROJECT_PATH="$1"
OUTPUT_PATH="$2"

if [ -z "$PROJECT_PATH" ] || [ -z "$OUTPUT_PATH" ]; then
    echo "用法: $0 <项目路径> <输出路径>"
    exit 1
fi

# 设置环境变量
export LITHO_LLM_API_KEY="your-api-key-here"

# 执行文档生成
litho \
  --project_path "$PROJECT_PATH" \
  --output_path "$OUTPUT_PATH" \
  --llm_provider openai \
  --model_efficient gpt-3.5-turbo \
  --verbose \
  --force_regenerate

if [ $? -eq 0 ]; then
    echo "✅ 文档生成成功: $OUTPUT_PATH"
else
    echo "❌ 文档生成失败"
    exit 1
fi
```

**最佳实践**:

- 在CI/CD流水线中集成文档生成步骤
- 使用环境变量管理敏感信息如API密钥
- 为不同环境配置不同的模型策略（开发用高效模型，生产用高质量模型）
- 定期运行文档生成以保持文档时效性

### API集成

如何在Rust应用中集成LLM客户端API

**示例代码**:

```
use deepwiki_rs::llm::client::{LLMClient, ReActConfig};
use serde::{Deserialize, Serialize};
use schemars::JsonSchema;

// 定义要提取的数据结构
#[derive(Serialize, Deserialize, JsonSchema)]
struct ArchitectureInsight {
    domain_modules: Vec<String>,
    core_components: Vec<String>,
    key_dependencies: Vec<String>,
    confidence_score: f64,
}

async fn analyze_project_structure() -> Result<(), Box<dyn std::error::Error>> {
    // 创建配置对象
    let config = create_llm_config();
    
    // 初始化LLM客户端
    let llm_client = LLMClient::new(config)?;
    
    // 结构化数据提取
    let insight: ArchitectureInsight = llm_client.extract(
        "你是一个专业的软件架构分析师",
        "分析以下代码库结构并提取关键架构信息..."
    ).await?;
    
    // 使用ReAct模式进行复杂推理
    let react_response = llm_client.prompt_with_react(
        "你是一个AI助手，可以使用工具帮助解决问题",
        "请分析这个项目的依赖关系并生成可视化图谱",
        ReActConfig::default(),
    ).await?;
    
    println!("核心模块: {:?}", insight.core_components);
    println!("ReAct执行轮数: {}", react_response.iterations_used);
    
    Ok(())
}

fn create_llm_config() -> Config {
    // 配置创建逻辑...
    unimplemented!()
}
```

**最佳实践**:

- 使用exponential backoff重试机制处理网络不稳定
- 实现合理的超时控制避免请求挂起
- 对敏感数据进行脱敏处理后再发送给LLM
- 监控LLM调用成本并设置预算告警
- 使用缓存避免重复的昂贵LLM调用

### 安全建议

使用DeepWiki-RS时的安全注意事项和最佳实践

**示例代码**:

```
# .env
# 生产环境API密钥管理
LITHO_LLM_API_KEY=sk-prod-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
LITHO_LLM_API_BASE_URL=https://api.openai.com/v1

# 开发环境使用模拟器或低权限密钥
# LITHO_LLM_API_KEY=sk-dev-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# 配置文件示例 (litho.toml)
[llm]
provider = "openai"
# 注意：不要在版本控制中硬编码API密钥
# api_key = "your-key-here"  # 不推荐！
api_base_url = "https://api.openai.com/v1"
model_efficient = "gpt-3.5-turbo"
model_powerful = "gpt-4-turbo"
```

**最佳实践**:

- 始终通过环境变量而非配置文件提供API密钥
- 使用最小权限原则配置LLM服务账号
- 对输入内容进行安全审查，防止提示注入攻击
- 限制可访问的文件路径，避免敏感文件泄露
- 定期轮换API密钥并监控异常使用模式
- 在企业环境中部署私有LLM网关进行流量审计


---

**分析置信度**: 9.5/10
